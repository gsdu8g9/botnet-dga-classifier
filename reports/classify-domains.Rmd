---
title: "Classify Domains"
author: "Nick Allen"
date: "October 8, 2014"
output:
  html_document:
    fig_height: 6
    fig_width: 8.5
    toc: yes
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
setwd ("..")
library (ProjectTemplate)
load.project ()
```

### Features

How well can we distinguish the classes based on the existing feature set?

```{r, results='hide'}
d <- domains [source %nin% c('alexa','quantcast')]
```

```{r}
ggplot (d, aes (dictionary, length, colour = type)) + geom_point()
```

### Classification

A rough pass at classifying domains as either legit or malicious based on the current feature set.  A portion of the data is held-out at random for cross-validation.

```{r, message=FALSE, results='hide', warning=FALSE}

# create a model with only a subset of all domains for expediancy
index <- createDataPartition(domains$type, p = .2, list = FALSE, times = 1) [, 1]
model <- train_classifier (type ~ length + dictionary, domains [index], p = 0.80)
```

```{r}
model
```

How accurate is the classifier?

```{r}
summary (model)
```

Are there any issues with either sensitivity or specificity?

```{r}
confusionMatrix (model, positive = "malicious")
```

### Real-World Classification

When such a model is applied in the real-world, the more challenging scenario is when a completely new malware implementation is encountered.  In this scenario there is no training data specific to the new maleware. The following assesses accuracy based on this more difficult scenario.

Step 1: Create a training data set using Goz and Cryptolocker.

```{r}
trainer <- domains [ source %in% c ("goz",
                                    "cryptolocker",
                                    "opendns-random-10k",
                                    "opendns-top-10k")]
```

Step 2: Train a model using all of the training data available from Goz and Cryptolocker.

```{r, message=FALSE, results='hide', warning=FALSE}
model <- train_classifier (type ~ length + dictionary, trainer, p = 1.0, n = 10)
```

```{r}
model
```

Step 3: Use the training results to select an alternate probability cutoff for class membership.

```{r}
# extract the model probabilities aka confidence
confidence <- predict (model, newdata = trainer, type = "prob") [["malicious"]]

# select an alternative cutoff based on the ROC
curve <- roc (response = trainer$type, predictor = confidence)
cutoff <- coords (curve, x = "best", best.method = "youden")
```

```{r}
cutoff
```

Step 4: Create a hold-out test data set as if Newgoz has never been encountered and is new in the wild.  Also, use a completely different set of 'legit' domains from Alexis.

```{r}
tester <- domains [ source %in% c ("newgoz", "alexa")]
```

Step 5: Use the model to determine the probability of being malicious.

```{r}
confidence <- predict (model, newdata = tester, type = "prob")
tester$type.prob <- confidence [["malicious"]]
```

Step 6: Use the model's probability and the alternative cutoff to perform final classification.

```{r}
tester [type.prob > cutoff[["threshold"]], type.hat := "malicious"]
tester [is.na (type.hat), type.hat := "legit"]
tester [, type.hat := as.factor (type.hat)]
```

### Diagnostics

What do the results look-like?

```{r}
with (tester, confusionMatrix (type, type.hat, positive = "malicious"))
```

Let's explore the trade-off between sensitivity and specificity.  First, capture the class probabilities from the model

```{r}

# extract the model probabilities aka confidence
confidence <- predict (model, newdata = tester, type = "prob")
names (confidence) <- c("legit.prob", "malicious.prob")

# attach them to the original data set
tester <- cbind (tester, confidence)
```

What do the confidence levels look-like for malicious domains?

```{r}
ggplot (tester [type == "malicious"], aes (malicious.prob)) + 
    geom_density () + 
    scale_x_continuous (labels=percent)
```

What do the confidence levels look-like for legit domains?

```{r}
ggplot (tester [type == "legit"], aes (legit.prob)) + 
    geom_density () + 
    scale_x_continuous (labels=percent)
```

Classify the (mis)prediction as a true positive, false positive, etc.

```{r, results='hide'}
tester [type == "malicious" & type.hat == "malicious", outcome := "True Positive"]
tester [type == "legit" & type.hat == "malicious", outcome := "False Positive"]
tester [type == "legit" & type.hat == "legit", outcome := "True Negative"]
tester [type == "malicious" & type.hat == "legit", outcome := "False Negative"]
```

What do the confidence levels look-like for each?

```{r}
ggplot (tester, aes (malicious.prob)) + 
    geom_histogram () +
    scale_x_continuous (labels=percent) +
    facet_wrap (~ outcome, scales = "free", ncol = 1)
```

Take a look at the Receiver Operating Characteristics.  Can adjusting the class probability cut-offs improve the results?

```{r}
curve <- roc (response = tester$type, predictor = tester$malicious.prob)
```

```{r}
plot (curve)
```

What would be the best cut-off based on the ROC?

```{r}
cutoff <- coords (curve, x = "best", best.method = "closest.topleft")
cutoff
```

What would the predictions now be based on the alternate cut-offs?

```{r}
tester [malicious.prob > cutoff["threshold"], type.hat.new := "malicious"]
tester [is.na (type.hat.new), type.hat.new := "legit"]
tester [, type.hat.new := factor (type.hat.new, levels = levels (type.hat))]

with (tester, confusionMatrix (type, type.hat.new, positive = "malicious"))
```

TODO: Ultimately, we cannot choose the cut-off based on held-out test data.  This has to be done on the training data alone.





